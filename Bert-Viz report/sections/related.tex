\section{Related Works}

There are plenty of research on BERT since it released with state-of-art performance on so many tasks.
~\cite{devlin2018bert} describe a method to check if the contextual embeddings capture the syntactic information in the sentence. They trained a transformation matrix between two embeddings and the squared L2 distance will correspond to their distance in parsing tree.
~\cite{coenen2019visualizing} find that the pre-trained BERT encode some syntatic and semantic information by probing BERT's geometry representations