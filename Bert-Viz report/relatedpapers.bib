@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{goldberg2019assessing,
  title={Assessing BERT's Syntactic Abilities},
  author={Goldberg, Yoav},
  journal={arXiv preprint arXiv:1901.05287},
  year={2019}
}

@techreport{wolf2019some,
  title={Some additional experiments extending the tech report “Assessing BERT’s Syntactic Abilities” by Yoav Goldberg},
  author={Wolf, Thomas},
  year={2019},
  institution={Technical report}
}

@article{ettinger2019bert,
  title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={arXiv preprint arXiv:1907.13528},
  year={2019}
}

@article{loureiro2019language,
  title={Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation},
  author={Loureiro, Daniel and Jorge, Alipio},
  journal={arXiv preprint arXiv:1906.10007},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@inproceedings{jawahar2019does,
  title={What does BERT learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e} and Unicomb, Samuel and I{\~n}iguez, Gerardo and Karsai, M{\'a}rton and L{\'e}o, Yannick and Karsai, M{\'a}rton and Sarraute, Carlos and Fleury, {\'E}ric and others},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics (ACL), Florence, Italy},
  year={2019}
}

@article{coenen2019visualizing,
  title={Visualizing and Measuring the Geometry of BERT},
  author={Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1906.02715},
  year={2019}
}

@article{jin2019probing,
  title={Probing biomedical embeddings from language models},
  author={Jin, Qiao and Dhingra, Bhuwan and Cohen, William W and Lu, Xinghua},
  journal={arXiv preprint arXiv:1904.02181},
  year={2019}
}

@article{peters2019tune,
  title={To tune or not to tune? adapting pretrained representations to diverse tasks},
  author={Peters, Matthew and Ruder, Sebastian and Smith, Noah A},
  journal={arXiv preprint arXiv:1903.05987},
  year={2019}
}

@article{alsentzer2019publicly,
  title={Publicly available clinical BERT embeddings},
  author={Alsentzer, Emily and Murphy, John R and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew},
  journal={arXiv preprint arXiv:1904.03323},
  year={2019}
}